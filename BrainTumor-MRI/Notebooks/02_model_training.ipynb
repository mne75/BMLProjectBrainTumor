{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "767e5569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SECTION 1: SUPERVISED CNN WITH TRANSFER LEARNING\n",
      "============================================================\n",
      "Found 2870 images belonging to 2 classes.\n",
      "Found 394 images belonging to 2 classes.\n",
      "\n",
      " Training samples: 2870\n",
      " Test samples: 394\n",
      " Classes: {'no_tumor': 0, 'tumor': 1}\n",
      "WARNING:tensorflow:From c:\\Users\\Warsan Musse\\.conda\\envs\\ml\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"Tumor_Classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 4, 4, 512)         14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               2097408   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16812353 (64.13 MB)\n",
      "Trainable params: 2097665 (8.00 MB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n",
      "\n",
      " Training supervised classifier...\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\Warsan Musse\\.conda\\envs\\ml\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Warsan Musse\\.conda\\envs\\ml\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "24/90 [=======>......................] - ETA: 3:18 - loss: 0.5590 - accuracy: 0.8451"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nMemoryError: Unable to allocate 8.24 MiB for an array with shape (32, 150, 150, 3) and data type float32\nTraceback (most recent call last):\n\n  File \"c:\\Users\\Warsan Musse\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"c:\\Users\\Warsan Musse\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"c:\\Users\\Warsan Musse\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"c:\\Users\\Warsan Musse\\.conda\\envs\\ml\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py\", line 917, in wrapped_generator\n    for data in generator_fn():\n\n  File \"c:\\Users\\Warsan Musse\\.conda\\envs\\ml\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py\", line 1064, in generator_fn\n    yield x[i]\n\n  File \"c:\\Users\\Warsan Musse\\.conda\\envs\\ml\\lib\\site-packages\\keras\\src\\preprocessing\\image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"c:\\Users\\Warsan Musse\\.conda\\envs\\ml\\lib\\site-packages\\keras\\src\\preprocessing\\image.py\", line 363, in _get_batches_of_transformed_samples\n    batch_x = np.zeros(\n\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 8.24 MiB for an array with shape (32, 150, 150, 3) and data type float32\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_2148]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Training supervised classifier...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     77\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Plot loss over epochs\u001b[39;00m\n\u001b[0;32m     80\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Warsan Musse\\.conda\\envs\\ml\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Warsan Musse\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nMemoryError: Unable to allocate 8.24 MiB for an array with shape (32, 150, 150, 3) and data type float32\nTraceback (most recent call last):\n\n  File \"c:\\Users\\Warsan Musse\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"c:\\Users\\Warsan Musse\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"c:\\Users\\Warsan Musse\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"c:\\Users\\Warsan Musse\\.conda\\envs\\ml\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py\", line 917, in wrapped_generator\n    for data in generator_fn():\n\n  File \"c:\\Users\\Warsan Musse\\.conda\\envs\\ml\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py\", line 1064, in generator_fn\n    yield x[i]\n\n  File \"c:\\Users\\Warsan Musse\\.conda\\envs\\ml\\lib\\site-packages\\keras\\src\\preprocessing\\image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"c:\\Users\\Warsan Musse\\.conda\\envs\\ml\\lib\\site-packages\\keras\\src\\preprocessing\\image.py\", line 363, in _get_batches_of_transformed_samples\n    batch_x = np.zeros(\n\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 8.24 MiB for an array with shape (32, 150, 150, 3) and data type float32\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_2148]"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# BRAIN TUMOR CLASSIFICATION & ANOMALY DETECTION\n",
    "# 02_model_training.ipynb\n",
    "# ============================================\n",
    "\n",
    "# Part 1: SUPERVISED LEARNING (VGG16)\n",
    "# Binary Classification: Tumor vs No Tumor\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SECTION 1: SUPERVISED CNN WITH TRANSFER LEARNING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "train_dir = \"../data/Training\"\n",
    "test_dir = \"../data/Testing\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_data = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\n Training samples: {train_data.samples}\")\n",
    "print(f\" Test samples: {test_data.samples}\")\n",
    "print(f\" Classes: {train_data.class_indices}\")\n",
    "\n",
    "# Load pretrained VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom layers\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "], name='Tumor_Classifier')\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train model\n",
    "print(\"\\n Training supervised classifier...\")\n",
    "history = model.fit(\n",
    "    train_data, \n",
    "    validation_data=test_data, \n",
    "    epochs=10,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot loss over epochs\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend()\n",
    "plt.title('Supervised Model Loss', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy over epochs\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.legend()\n",
    "plt.title('Supervised Model Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/supervised_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save model\n",
    "model.save(\"../models/tumor_classifier_vgg16.keras\")\n",
    "print(\"\\n Supervised model saved: tumor_classifier_vgg16.keras\")\n",
    "print(f\" Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# SECTION 2: UNSUPERVISED LEARNING (AUTOENCODER)\n",
    "# Anomaly Detection: Trained only on healthy images\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SECTION 2: UNSUPERVISED AUTOENCODER FOR ANOMALY DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Data augmentation for autoencoder\n",
    "train_datagen_ae = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load ONLY healthy images (no_tumor) for unsupervised learning\n",
    "train_data_healthy = train_datagen_ae.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='input',  # Autoencoder: output = input\n",
    "    classes=['no_tumor'],  # ONLY healthy brains!\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"\\n Healthy training samples: {train_data_healthy.samples}\")\n",
    "\n",
    "# Build Convolutional Autoencoder\n",
    "def build_autoencoder(input_shape=(150, 150, 3)):\n",
    "    \"\"\"Convolutional Autoencoder for image reconstruction\"\"\"\n",
    "    \n",
    "    input_img = layers.Input(shape=input_shape, name='input')\n",
    "    \n",
    "    # ENCODER\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)  # 75x75\n",
    "    \n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)  # 37x37\n",
    "    \n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    encoded = layers.MaxPooling2D((2, 2), padding='same')(x)  # 19x19\n",
    "    \n",
    "    # DECODER\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)  # 38x38\n",
    "    \n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)  # 76x76\n",
    "    \n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)  # 152x152\n",
    "    \n",
    "    # Crop to 150x150 and reconstruct RGB\n",
    "    x = layers.Cropping2D(cropping=((1, 1), (1, 1)))(x)\n",
    "    decoded = layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same', name='output')(x)\n",
    "    \n",
    "    return models.Model(input_img, decoded, name='Autoencoder')\n",
    "\n",
    "# Build and compile\n",
    "autoencoder = build_autoencoder()\n",
    "autoencoder.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "autoencoder.summary()\n",
    "\n",
    "# Callbacks\n",
    "callbacks_ae = [\n",
    "    EarlyStopping(monitor='loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
    "]\n",
    "\n",
    "# Train autoencoder\n",
    "print(\"\\n Training autoencoder on healthy images only...\")\n",
    "history_ae = autoencoder.fit(\n",
    "    train_data_healthy,\n",
    "    epochs=50,\n",
    "    steps_per_epoch=len(train_data_healthy),\n",
    "    callbacks=callbacks_ae,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_ae.history['loss'], label='Training Loss (MSE)', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Autoencoder Training Loss', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_ae.history['mae'], label='Training MAE', linewidth=2, color='orange')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('MAE', fontsize=12)\n",
    "plt.title('Mean Absolute Error', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/autoencoder_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… Final training loss: {history_ae.history['loss'][-1]:.6f}\")\n",
    "\n",
    "# Test reconstruction quality\n",
    "train_data_healthy.reset()\n",
    "sample_images, _ = next(train_data_healthy)\n",
    "reconstructed = autoencoder.predict(sample_images[:5], verbose=0)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i in range(5):\n",
    "    # Original\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(sample_images[i])\n",
    "    plt.title('Original', fontsize=10)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Reconstructed\n",
    "    plt.subplot(2, 5, i + 6)\n",
    "    plt.imshow(reconstructed[i])\n",
    "    error = np.mean(np.square(sample_images[i] - reconstructed[i]))\n",
    "    plt.title(f'Reconstructed\\nMSE: {error:.4f}', fontsize=10)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle('Reconstruction Quality on Healthy Brains', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/reconstruction_samples.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate threshold for anomaly detection\n",
    "print(\"\\n Calculating optimal threshold...\")\n",
    "\n",
    "train_data_healthy.reset()\n",
    "all_healthy = []\n",
    "for i in range(len(train_data_healthy)):\n",
    "    batch, _ = next(train_data_healthy)\n",
    "    all_healthy.append(batch)\n",
    "all_healthy = np.concatenate(all_healthy)\n",
    "\n",
    "reconstructed_healthy = autoencoder.predict(all_healthy, verbose=0)\n",
    "errors_healthy = np.mean(np.square(all_healthy - reconstructed_healthy), axis=(1, 2, 3))\n",
    "\n",
    "mean_error = np.mean(errors_healthy)\n",
    "std_error = np.std(errors_healthy)\n",
    "threshold_95 = np.percentile(errors_healthy, 95)\n",
    "threshold_99 = np.percentile(errors_healthy, 99)\n",
    "\n",
    "print(f\"\\n Reconstruction Error Statistics (healthy images):\")\n",
    "print(f\"   Mean: {mean_error:.6f}\")\n",
    "print(f\"   Std:  {std_error:.6f}\")\n",
    "print(f\"   95th percentile: {threshold_95:.6f}\")\n",
    "print(f\"   99th percentile: {threshold_99:.6f}\")\n",
    "\n",
    "# Save threshold\n",
    "np.save('../models/anomaly_threshold.npy', threshold_95)\n",
    "print(f\"\\n Threshold saved: {threshold_95:.6f}\")\n",
    "\n",
    "# Plot error distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(errors_healthy, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(threshold_95, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'95th percentile: {threshold_95:.4f}')\n",
    "plt.axvline(mean_error, color='green', linestyle='--', linewidth=2, \n",
    "            label=f'Mean: {mean_error:.4f}')\n",
    "plt.xlabel('Reconstruction Error (MSE)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Error Distribution on Healthy Images', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/error_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save autoencoder\n",
    "autoencoder.save(\"../models/autoencoder_abnormality.keras\")\n",
    "print(\"\\n Autoencoder saved: autoencoder_abnormality.keras\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nModels created:\")\n",
    "print(\"  1. tumor_classifier_vgg16.keras (Supervised)\")\n",
    "print(\"  2. autoencoder_abnormality.keras (Unsupervised)\")\n",
    "print(\"  3. anomaly_threshold.npy (Threshold for detection)\")\n",
    "print(\"\\n Next step: Run 03_evaluation.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
